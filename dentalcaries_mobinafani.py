# -*- coding: utf-8 -*-
"""DentalCaries-MobinaFani

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P7Ews3Ch1z1020zx1Hj1mh3LxtKDCakz

## **Connect to Google Drive**
"""

from google.colab import drive
drive.mount('/content/drive')

"""## **Import libraries**"""

import os
from os import listdir
import tensorflow as tf
from tensorflow import keras
from keras.preprocessing import image
import matplotlib.pyplot as plt 
import numpy as np
from keras.utils.np_utils import to_categorical
import random,shutil
from keras.models import Sequential
from keras.layers import Dropout,Conv2D,Flatten,Dense, MaxPooling2D, BatchNormalization
from keras.models import load_model
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.python.keras import models
from tensorflow.python.keras import layers
from tensorflow.keras import optimizers
from tensorflow.keras.applications import VGG16
from tensorflow.keras import regularizers

"""## **Read Images and Resize**"""

data_list =listdir('/content/drive/MyDrive/Dental_dir')
print(len(data_list))
train_dir  = '/content/drive/MyDrive/Dental_dir/Train_dir'
test_dir =  '/content/drive/MyDrive/Dental_dir/Test_dir'
val_dir = '/content/drive/MyDrive/Dental_dir/Validation_dir'

IMAGE_SIZE    = (230,230)
NUM_CLASSES   = len(data_list)
batch_size    = 20  # try reducing batch size or freeze more layers if your GPU runs out of memory

#Train datagen here is a preprocessor
train_datagen = ImageDataGenerator(rescale=1./255,
                                    rotation_range=2,
                                    featurewise_center = False,
                                   featurewise_std_normalization = False,
                                    width_shift_range=0.05,
                                    height_shift_range=0.05,
                                    shear_range=0.05,
                                    zoom_range=0.05,
)
val_datagen = ImageDataGenerator(rescale=1./255,
                                    rotation_range=2,
                                   featurewise_center = False,
                                    featurewise_std_normalization = False,
                                    width_shift_range=0.05,
                                    height_shift_range=0.05,
                                    shear_range=0.05,
                                    zoom_range=0.05,
)
test_datagen = ImageDataGenerator(rescale=1./255)

train_batches = train_datagen.flow_from_directory(train_dir,
                                                  target_size=IMAGE_SIZE,
                                                  shuffle=True,
                                                  batch_size=batch_size,
                                                  seed=42,
                                                  class_mode="binary"   #For multiclass use categorical n for binary use binary
                                                  )
valid_batches = val_datagen.flow_from_directory(val_dir,
                                                  target_size=IMAGE_SIZE,
                                                  shuffle=True,
                                                  batch_size=batch_size,
                                                  seed=42,
                                                  class_mode="binary"  #For multiclass use categorical n for binary use binary
                                                  )
test_batches = test_datagen.flow_from_directory(test_dir,
                                                  target_size=IMAGE_SIZE,
                                                  shuffle=True,
                                                  batch_size=1,
                                                  seed=42,
                                                  class_mode="binary"  #For multiclass use categorical n for binary use binary
                                                  )

"""## **Neural Network Architecture**"""

model = models.Sequential()
model.add(VGG16(input_shape=(230,230 ,3) ,include_top=False,pooling='max', weights='imagenet'))
model.add(layers.Dropout(0.5))
model.add(layers.Dense(1024, activation='elu'))
model.add(layers.Dropout(0.5))
model.add(layers.Dense(2048, kernel_regularizer= regularizers.l1(0.001), activation='elu'))
model.add(layers.Dropout(0.5))
model.add(layers.Dense(1, activation='sigmoid'))

model.layers[0].trainable = False
model.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])

"""## **Train The Network**"""

result=model.fit(train_batches, steps_per_epoch= int(240//batch_size), validation_data = valid_batches, validation_steps=int(70//batch_size), epochs= 24)
result.history.keys()
acc = result.history['accuracy']
print (sum(acc) / len(acc))
val_acc =result.history['val_accuracy']
loss = result.history['loss']
val_loss = result.history['val_loss']

"""## **Summary Of Model**"""

model.summary()

"""## **Show Plot**"""

epochs = range(1, len(acc) + 1)
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs,val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()
plt.figure()
plt.plot(epochs,loss , 'bo', label='Training loss')
plt.plot(epochs,val_loss , 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()
plt.show()

